import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.nn.utils import weight_norm  # æ·»åŠ æƒé‡å½’ä¸€åŒ–æ”¯æŒ

class MultiLevelMLP(nn.Module):
    """å¤šå±‚çº§MLPç³»ç»Ÿ - ç”¨äºç”Ÿæˆä¸‰ä¸ªå±‚çº§çš„promptä¸“å®¶è·¯ç”±æƒé‡"""
    def __init__(self, embed_dim=96, n_experts=8, d_moe_low=None):
        super().__init__()
        self.embed_dim = embed_dim
        self.n_experts = n_experts
        
        # ä¿®æ”¹å¤„ï¼šå…ˆä¸åˆå§‹åŒ–å…·ä½“çš„Linearå±‚ï¼Œåœ¨ç¬¬ä¸€æ¬¡forwardæ—¶åŠ¨æ€åˆ›å»º
        self.d_moe_low = d_moe_low
        self.initialized = False
        
        # å­˜å‚¨ç»´åº¦ä¿¡æ¯ï¼Œåœ¨é¦–æ¬¡forwardæ—¶è®¾ç½®
        self.low_feat_dim = None
        self.med_feat_dim = embed_dim
        self.high_feat_dim = embed_dim
        self.total_feat_dim = None
        
    def _initialize_networks(self, total_feat_dim, low_feat_dim, device):
        """é¦–æ¬¡forwardæ—¶åŠ¨æ€åˆå§‹åŒ–ç½‘ç»œ"""
        self.total_feat_dim = total_feat_dim
        self.low_feat_dim = low_feat_dim
    
        print(f"DEBUG - Initializing MLPs with total_feat_dim: {total_feat_dim}, low_feat_dim: {low_feat_dim}")
    
        # ä¿®æ”¹å¤„ï¼šç¡®å®šè®¾å¤‡ä½ç½®
        # ä»å½“å‰æ¨¡å—çš„å·²æœ‰å‚æ•°è·å–è®¾å¤‡ï¼Œå¦‚æœæ²¡æœ‰å‚æ•°åˆ™ä½¿ç”¨cuda
        device = next(self.parameters()).device if len(list(self.parameters())) > 0 else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
        # åŠ¨æ€åˆ›å»ºMLPç½‘ç»œå¹¶ç›´æ¥æ”¾åˆ°æ­£ç¡®è®¾å¤‡ä¸Šï¼Œä½¿ç”¨weight_normç¨³å®šè®­ç»ƒ
        self.mlp_low = nn.Sequential(
            weight_norm(nn.Linear(total_feat_dim, self.embed_dim * 2)),  
            nn.GELU(),
            nn.Dropout(0.1),
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),  
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(), 
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim, self.n_experts)),
            nn.Softmax(dim=-1)
        ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡

        self.mlp_medium = nn.Sequential(
            weight_norm(nn.Linear(total_feat_dim, self.embed_dim * 2)),  
            nn.GELU(),
            nn.Dropout(0.1),
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),  
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(), 
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim, self.n_experts)),
            nn.Softmax(dim=-1)
        ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡

        self.mlp_high = nn.Sequential(
            weight_norm(nn.Linear(total_feat_dim, self.embed_dim * 2)),  
            nn.GELU(),
            nn.Dropout(0.1),
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),  
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(), 
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim, self.n_experts)),
            nn.Softmax(dim=-1)
        ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡

        # é‡æ„ç½‘ç»œ
        self.reconstructor_low = nn.Sequential(
            weight_norm(nn.Linear(self.n_experts, self.embed_dim * 2)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),  # è¾“å‡ºç»´åº¦ä¸ºä½å±‚çº§ç‰¹å¾ç»´åº¦çš„ä¸¤å€
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(self.embed_dim, low_feat_dim)
            ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡

        self.reconstructor_med = nn.Sequential(
            weight_norm(nn.Linear(self.n_experts, self.embed_dim * 2)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),  # è¾“å‡ºç»´åº¦ä¸ºä½å±‚çº§ç‰¹å¾ç»´åº¦çš„ä¸¤å€
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ 
            nn.Linear(self.embed_dim, self.med_feat_dim)
        ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡

        self.reconstructor_high = nn.Sequential(
            weight_norm(nn.Linear(self.n_experts, self.embed_dim * 2)),
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),  # è¾“å‡ºç»´åº¦ä¸ºä½å±‚çº§ç‰¹å¾ç»´åº¦çš„ä¸¤å€
            nn.GELU(),
            nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 4, self.embed_dim * 2)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            # weight_norm(nn.Linear(self.embed_dim * 2, self.embed_dim)),
            # nn.GELU(),
            # nn.Dropout(0.1),  # æ·»åŠ Dropoutå±‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(self.embed_dim, self.high_feat_dim)
        ).to(device)  # ä¿®æ”¹å¤„ï¼šç›´æ¥åˆ›å»ºæ—¶å°±æ”¾åˆ°æ­£ç¡®è®¾å¤‡
    
        print(f"DEBUG - Networks created on device: {device}")  # ä¿®æ”¹å¤„ï¼šæ·»åŠ è®¾å¤‡ç¡®è®¤
    
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        self.initialized = True
        
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
    def forward(self, feat_low, feat_med, feat_high):
        """
        Args:
            feat_low: [B, low_feat_dim] ä½å±‚çº§ç‰¹å¾ - å›¾æ–‡æŠ•å½±æ‹¼æ¥
            feat_med: [B, embed_dim] ä¸­å±‚çº§ç‰¹å¾ - CNNæ± åŒ–
            feat_high: [B, embed_dim] é«˜å±‚çº§ç‰¹å¾ - CNNæ± åŒ–
        Returns:
            dict: åŒ…å«è·¯ç”±åˆ†æ•°å’Œä¸­é—´ç‰¹å¾çš„å­—å…¸
        """
        B = feat_low.shape[0]
    
        # ä¿®æ”¹å¤„ï¼šæ‹¼æ¥æ‰€æœ‰å±‚çº§ç‰¹å¾
        concat_features = torch.cat([feat_low, feat_med, feat_high], dim=-1)
        
        # ä¿®æ”¹å¤„ï¼šå¯¹æ‹¼æ¥åçš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…æ•°å€¼è¿‡å¤§å¯¼è‡´ä¸ç¨³å®š
        concat_features = F.normalize(concat_features, dim=-1)
    
        # ä¿®æ”¹å¤„ï¼šé¦–æ¬¡forwardæ—¶åŠ¨æ€åˆå§‹åŒ–ï¼Œä»è¾“å…¥å¼ é‡è·å–è®¾å¤‡
        if not self.initialized:
            total_feat_dim = concat_features.shape[-1]
            low_feat_dim = feat_low.shape[-1]
            input_device = feat_low.device  # ä»è¾“å…¥è·å–è®¾å¤‡
            self._initialize_networks(total_feat_dim, low_feat_dim, input_device)
    
        # è·¨å±‚çº§ä»»åŠ¡
        low_scores_cross = self.mlp_low(concat_features)
        med_scores_cross = self.mlp_medium(concat_features)
        high_scores_cross = self.mlp_high(concat_features)
    
        # è‡ªèº«å±‚çº§ä»»åŠ¡
        noise_scale = 0.1
    
        # ä½å±‚çº§è‡ªèº«ä»»åŠ¡
        low_self_input = torch.cat([
            feat_low,
            torch.randn(B, self.med_feat_dim, device=feat_low.device) * noise_scale,
            torch.randn(B, self.high_feat_dim, device=feat_low.device) * noise_scale
        ], dim=-1)
        low_scores_self = self.mlp_low(low_self_input)
    
        # ä¸­å±‚çº§è‡ªèº«ä»»åŠ¡
        med_self_input = torch.cat([
            torch.randn(B, self.low_feat_dim, device=feat_med.device) * noise_scale,
            feat_med,
            torch.randn(B, self.high_feat_dim, device=feat_med.device) * noise_scale
        ], dim=-1)
        med_scores_self = self.mlp_medium(med_self_input)
    
        # é«˜å±‚çº§è‡ªèº«ä»»åŠ¡
        high_self_input = torch.cat([
            torch.randn(B, self.low_feat_dim, device=feat_high.device) * noise_scale,
            torch.randn(B, self.med_feat_dim, device=feat_high.device) * noise_scale,
            feat_high
        ], dim=-1)
        high_scores_self = self.mlp_high(high_self_input)
    
        return {
            'low_cross': low_scores_cross,
            'med_cross': med_scores_cross, 
            'high_cross': high_scores_cross,
            'low_self': low_scores_self,
            'med_self': med_scores_self,
            'high_self': high_scores_self,
            'features': {
                'low': feat_low,
                'med': feat_med, 
                'high': feat_high,
                'concat': concat_features
            }
        }

class FeatureLossComputer(nn.Module):
    """ç‰¹å¾æŸå¤±è®¡ç®—å™¨ - å®ç°feature_crossing_losså’Œfeature_self_loss"""
    def __init__(self, embed_dim=96):
        super().__init__()
        self.embed_dim = embed_dim
        
        # ä¿®æ”¹å¤„ï¼šæ›´ä¿å®ˆçš„è¶…å‚æ•°è®¾ç½®ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        self.lambda1 = 0.1   # å¤šæ ·æ€§æŸå¤±æƒé‡ (ä»0.3é™åˆ°0.1)
        self.lambda2 = 0.05  # å¹³è¡¡æŸå¤±æƒé‡ (ä»0.2é™åˆ°0.05)
        self.gamma1 = 0.2    # ä¸“æ³¨æŸå¤±æƒé‡ (ä»0.4é™åˆ°0.2)
        self.gamma2 = 0.1    # ä¸€è‡´æ€§æŸå¤±æƒé‡ (ä»0.3é™åˆ°0.1)
        self.tau = 1.0      # å¤šæ ·æ€§é˜ˆå€¼ (ä»1.0é™åˆ°0.5)
        
        # ä¿®æ”¹å¤„ï¼šå¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ï¼Œåˆå§‹å€¼æ›´å¤§ä»¥å¢åŠ ç¨³å®šæ€§
        self.temperature = nn.Parameter(torch.tensor(1.0))  # ä»0.07æé«˜åˆ°1.0
        
    def feature_crossing_loss(self, features):
        """
        é‡æ–°å®šä¹‰è·¨å±‚çº§åè°ƒæŸå¤±ï¼š
        - ä»…åŒ…å«å¤šæ ·æ€§æŸå¤±ï¼Œé˜²æ­¢ä¸åŒå±‚çº§ç‰¹å¾è¶‹åŒ
        - å¯¹è¾“å…¥ç‰¹å¾ detachï¼Œé¿å…æ¢¯åº¦å›ä¼ åˆ°æå–å™¨
        L_cross = Î» * diversity_loss
        """
        # detach ç‰¹å¾ä»¥é˜»æ–­æ¢¯åº¦ä¼ æ’­åˆ°æå–å™¨
        feat_low = features['low'].detach()
        feat_med = features['med'].detach()
        feat_high = features['high'].detach()

        # ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´ï¼Œå…ˆå°†ä½ç»´ç‰¹å¾è‡ªé€‚åº”æ± åŒ–åˆ°ä¸­ç»´åº¦(embed_dim)ï¼Œä¿è¯ä¸‰è€…ç»´åº¦ä¸€è‡´
        # è¿™é‡Œ med_feat_dim == high_feat_dim == embed_dim
        med_feat_dim = self.embed_dim
        feat_low_proj = F.adaptive_avg_pool1d(feat_low.unsqueeze(1), med_feat_dim).squeeze(1)
        # è®¡ç®—å¤šæ ·æ€§æŸå¤±ï¼šæƒ©ç½šè¿‡åº¦ç›¸ä¼¼çš„ç‰¹å¾å¯¹ï¼ˆL1è·ç¦»ï¼‰
        d_lm = torch.mean(torch.abs(feat_low_proj - feat_med))
        d_lh = torch.mean(torch.abs(feat_low_proj - feat_high))
        d_mh = torch.mean(torch.abs(feat_med - feat_high))
        diversity_loss = (F.softplus(self.tau - d_lm) + F.softplus(self.tau - d_lh) + F.softplus(self.tau - d_mh)) / 3

        # åŠ æƒè¾“å‡º diversity loss
        total_crossing_loss = self.lambda1 * diversity_loss
         
        # è®¡ç®—è·¨å±‚çº§åè°ƒæŸå¤± L_crossing - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„ç¨³å®šç‰ˆæœ¬
        
        # æ•°å­¦å®šä¹‰ï¼š
        # L_crossing = L_correlation + Î»â‚ * L_diversity + Î»â‚‚ * L_balance
        
        # Args:
        #     features: dictåŒ…å«'low', 'med', 'high'ä¸‰ä¸ªå±‚çº§çš„ç‰¹å¾
        # Returns:
        #     torch.Tensor: æ ‡é‡æŸå¤±å€¼
        # """
        # feat_low = features['low']    # [B, embed_dim]
        # feat_med = features['med']    # [B, embed_dim]  
        # feat_high = features['high']  # [B, embed_dim]
        
        # # æ·»åŠ æ¢¯åº¦æ£€æŸ¥å’Œè¾“å‡ºå‡½æ•°
        # def check_tensor_stability(tensor, name, print_details=False):
        #     """æ£€æŸ¥å¼ é‡çš„æ•°å€¼ç¨³å®šæ€§"""
        #     if torch.isnan(tensor).any():
        #         print(f"âŒ è­¦å‘Š: {name} åŒ…å«NaNå€¼")
        #         return False
        #     if torch.isinf(tensor).any():
        #         print(f"âŒ è­¦å‘Š: {name} åŒ…å«æ— ç©·å¤§å€¼")
        #         return False
        #     if tensor.abs().max() > 1e6:
        #         print(f"âŒ è­¦å‘Š: {name} åŒ…å«è¿‡å¤§å€¼ï¼Œæœ€å¤§å€¼: {tensor.abs().max()}")
        #         return False
        #     if print_details:
        #         print(f"âœ… {name}: min={tensor.min():.6f}, max={tensor.max():.6f}, mean={tensor.mean():.6f}, std={tensor.std():.6f}")
        #     return True

        # # è¾“å…¥ç‰¹å¾é¢„å¤„ç† - å¼ºåˆ¶æ¢¯åº¦è£å‰ªå’Œå½’ä¸€åŒ–
        # def preprocess_features(x, name):
        #     """é¢„å¤„ç†ç‰¹å¾ï¼Œç¡®ä¿æ•°å€¼ç¨³å®š"""
        #     # æ¢¯åº¦è£å‰ª
        #     x = torch.clamp(x, -10.0, 10.0)
        #     # L2å½’ä¸€åŒ–
        #     x = F.normalize(x, p=2, dim=-1, eps=1e-8)
        #     # å†æ¬¡æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢å½’ä¸€åŒ–åçš„å¼‚å¸¸
        #     x = torch.clamp(x, -1.0, 1.0)
        #     # check_tensor_stability(x, f"preprocessed_{name}")
        #     return x
        
        # # é¢„å¤„ç†æ‰€æœ‰ç‰¹å¾
        # feat_low = preprocess_features(feat_low, "feat_low")
        # feat_med = preprocess_features(feat_med, "feat_med") 
        # feat_high = preprocess_features(feat_high, "feat_high")
        
        # # ç¨³å®šçš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆæ›¿ä»£Pearsonç›¸å…³ç³»æ•°ï¼‰
        # def stable_cosine_similarity(x, y):
        #     """
        #     ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ›¿ä»£Pearsonç›¸å…³ç³»æ•°ï¼Œæ›´åŠ ç¨³å®š
        #     """
        #     # å¤„ç†ç»´åº¦ä¸åŒ¹é…
        #     if x.shape[-1] != y.shape[-1]:
        #         min_dim = min(x.shape[-1], y.shape[-1])
        #         x = x[:, :min_dim]
        #         y = y[:, :min_dim]
            
        #     # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - è¾“å…¥å·²å½’ä¸€åŒ–
        #     similarity = F.cosine_similarity(x, y, dim=-1).mean()
            
        #     # ä½¿ç”¨æ¸©åº¦å‚æ•°å¹³æ»‘
        #     similarity = similarity / self.temperature
            
        #     # è£å‰ªåˆ°å®‰å…¨èŒƒå›´
        #     similarity = torch.clamp(similarity, -10.0, 10.0)
            
        #     return similarity
        
        # # è®¡ç®—ç›¸å…³æ€§æŸå¤±
        # corr_low_med = stable_cosine_similarity(feat_low, feat_med)
        # corr_low_high = stable_cosine_similarity(feat_low, feat_high)
        # corr_med_high = stable_cosine_similarity(feat_med, feat_high)
        
        # # æ£€æŸ¥ç›¸å…³ç³»æ•°çš„ç¨³å®šæ€§
        # # check_tensor_stability(corr_low_med, "corr_low_med", True)
        # # check_tensor_stability(corr_low_high, "corr_low_high", True)
        # # check_tensor_stability(corr_med_high, "corr_med_high", True)
        
        # # ç›®æ ‡ï¼šé€‚åº¦æ­£ç›¸å…³ï¼ˆç»è¿‡æ¸©åº¦ç¼©æ”¾åçš„ç›®æ ‡å€¼ï¼‰
        # target_corr = 0.3 / self.temperature.item()  # è€ƒè™‘æ¸©åº¦å‚æ•°
        
        # # ä½¿ç”¨HuberæŸå¤±æ›¿ä»£MSEï¼Œå‡å°‘å¤§è¯¯å·®çš„æ¢¯åº¦çˆ†ç‚¸
        # def huber_loss(pred, target, delta=1.0):
        #     """HuberæŸå¤±å‡½æ•°ï¼Œå¯¹å¤§è¯¯å·®æ›´ç¨³å®š"""
        #     error = pred - target
        #     abs_error = torch.abs(error)
        #     return torch.where(
        #         abs_error <= delta,
        #         0.5 * error * error,
        #         delta * (abs_error - 0.5 * delta)
        #     )
        
        # correlation_loss = (
        #     huber_loss(corr_low_med, target_corr, delta=0.5) + 
        #     huber_loss(corr_low_high, target_corr, delta=0.5) + 
        #     huber_loss(corr_med_high, target_corr, delta=0.5)
        # ) / 3
        
        # # ç¨³å®šçš„è·ç¦»è®¡ç®—
        # def stable_distance(x, y):
        #     """è®¡ç®—ç¨³å®šçš„è·ç¦»"""
        #     if x.shape[-1] != y.shape[-1]:
        #         min_dim = min(x.shape[-1], y.shape[-1])
        #         x = x[:, :min_dim]
        #         y = y[:, :min_dim]
            
        #     # ä½¿ç”¨L1è·ç¦»ï¼Œæ¯”L2æ›´ç¨³å®š
        #     distance = torch.mean(torch.abs(x - y))
        #     return torch.clamp(distance, 0, 2.0)
        
        # dist_low_med = stable_distance(feat_low, feat_med)
        # dist_low_high = stable_distance(feat_low, feat_high)
        # dist_med_high = stable_distance(feat_med, feat_high)
        
        # # å¤šæ ·æ€§æŸå¤± - ä½¿ç”¨å¹³æ»‘çš„ReLUæ›¿ä»£
        # tau_normalized = 0.1  # é™ä½é˜ˆå€¼ï¼Œå› ä¸ºä½¿ç”¨äº†L1è·ç¦»
        # diversity_loss = (
        #     F.softplus(tau_normalized - dist_low_med) + 
        #     F.softplus(tau_normalized - dist_low_high) + 
        #     F.softplus(tau_normalized - dist_med_high)
        # ) / 3
        
        # # ç®€åŒ–çš„å¹³è¡¡æŸå¤±
        # def stable_feature_norm(x):
        #     """è®¡ç®—ç¨³å®šçš„ç‰¹å¾èŒƒæ•°"""
        #     # ä½¿ç”¨L1èŒƒæ•°çš„å‡å€¼ï¼Œæ›´ç¨³å®š
        #     norm = torch.mean(torch.abs(x))
        #     return torch.clamp(norm, 1e-8, 1.0)  # è¾“å…¥å·²å½’ä¸€åŒ–ï¼ŒèŒƒå›´åº”è¯¥å¾ˆå°
        
        # norm_low = stable_feature_norm(feat_low)
        # norm_med = stable_feature_norm(feat_med)
        # norm_high = stable_feature_norm(feat_high)
        
        # # ä½¿ç”¨ç›¸å¯¹æ ‡å‡†å·®ä½œä¸ºå¹³è¡¡æŸå¤±
        # norms = torch.stack([norm_low, norm_med, norm_high])
        # mean_norm = norms.mean()
        # relative_std = torch.std(norms) / (mean_norm + 1e-8)
        # balance_loss = torch.clamp(relative_std, 0, 1.0)
        
        # # æ£€æŸ¥å„æŸå¤±ç»„ä»¶çš„ç¨³å®šæ€§
        # # check_tensor_stability(correlation_loss, "correlation_loss", True)
        # # check_tensor_stability(diversity_loss, "diversity_loss", True) 
        # # check_tensor_stability(balance_loss, "balance_loss", True)
        
        # # ä½¿ç”¨æ›´ä¿å®ˆçš„æƒé‡ç»„åˆ
        # safe_lambda1 = torch.clamp(torch.tensor(self.lambda1), 0, 0.1)  # é™åˆ¶æƒé‡èŒƒå›´
        # safe_lambda2 = torch.clamp(torch.tensor(self.lambda2), 0, 0.1)
        
        # total_crossing_loss = (
        #     correlation_loss + 
        #     safe_lambda1 * diversity_loss + 
        #     safe_lambda2 * balance_loss
        # )
        
        # # æœ€ç»ˆç¨³å®šæ€§ä¿è¯
        # total_crossing_loss = torch.clamp(total_crossing_loss, 0, 10.0)  # æ›´ä¸¥æ ¼çš„ä¸Šé™
        
        # # è¯¦ç»†çš„è°ƒè¯•è¾“å‡º
        # if torch.isnan(total_crossing_loss) or torch.isinf(total_crossing_loss) or total_crossing_loss > 5.0:
        #     print(f"ğŸš¨ CROSSING LOSS å¼‚å¸¸æ£€æµ‹:")
        #     print(f"  correlation_loss: {correlation_loss.item():.6f}")
        #     print(f"  diversity_loss: {diversity_loss.item():.6f}")  
        #     print(f"  balance_loss: {balance_loss.item():.6f}")
        #     print(f"  total_crossing_loss: {total_crossing_loss.item():.6f}")
        #     print(f"  ç›¸å…³ç³»æ•°: low_med={corr_low_med.item():.6f}, low_high={corr_low_high.item():.6f}, med_high={corr_med_high.item():.6f}")
        #     print(f"  è·ç¦»: low_med={dist_low_med.item():.6f}, low_high={dist_low_high.item():.6f}, med_high={dist_med_high.item():.6f}")
        #     print(f"  èŒƒæ•°: low={norm_low.item():.6f}, med={norm_med.item():.6f}, high={norm_high.item():.6f}")
            
        return total_crossing_loss
    
    def feature_self_loss(self, mlp_outputs, mlp_module):
        """
        è®¡ç®—è‡ªèº«å±‚çº§å…³æ³¨æŸå¤± L_self
        
        æ•°å­¦å®šä¹‰ï¼š
        L_self = Î£â‚– [L_recon^k + Î³â‚ * L_focus^k + Î³â‚‚ * L_consistency^k]
        
        Args:
            mlp_outputs: MultiLevelMLPçš„è¾“å‡ºå­—å…¸
            mlp_module: MultiLevelMLPæ¨¡å—å®ä¾‹
        Returns:
            torch.Tensor: æ ‡é‡æŸå¤±å€¼
        """
        features = mlp_outputs['features']
        total_loss = 0
        
        for level in ['low', 'med', 'high']:
            # ä¿®æ”¹å¤„ï¼šé‡æ„æŸå¤± - æµ‹è¯•MLPä»å™ªå£°ä¸­æ¢å¤è‡ªèº«ç‰¹å¾çš„èƒ½åŠ›
            self_scores = mlp_outputs[f'{level}_self']  # [B, n_experts]
            original_feat = features[level]  # [B, embed_dim]
            
            # ä½¿ç”¨å¯¹åº”çš„é‡æ„å™¨
            reconstructor = getattr(mlp_module, f'reconstructor_{level}')
            reconstructed_feat = reconstructor(self_scores)  # [B, embed_dim]
            
            # MSEé‡æ„æŸå¤±ï¼š||Decoder(MLP(F_k + N)) - F_k||â‚‚Â²
            recon_loss = F.mse_loss(reconstructed_feat, original_feat)
            
            # ä¿®æ”¹å¤„ï¼šä¸“æ³¨æŸå¤± - ç¡®ä¿MLPå¯¹è‡ªèº«å±‚çº§æœ€æ•æ„Ÿ
            cross_scores = mlp_outputs[f'{level}_cross']  # [B, n_experts]
            
            # # è®¡ç®—è‡ªèº«è¾“å…¥çš„æœ€å¤§å“åº” vs æ··åˆè¾“å…¥çš„å¹³å‡å“åº”
            # max_self_response = torch.max(self_scores, dim=-1)[0].mean()  # æ ‡é‡
            # avg_cross_response = cross_scores.mean()  # æ ‡é‡
            
            # ä¸“æ³¨æŸå¤±ï¼š-log(max_self / avg_cross)
            # ç›®æ ‡ï¼šè‡ªèº«è¾“å…¥åº”è¯¥äº§ç”Ÿæ›´å¼ºçš„å“åº”
            # focus_loss = -torch.log(max_self_response / (avg_cross_response + 1e-8))
            
            # # ä¿®æ”¹å¤„ï¼šä¸€è‡´æ€§æŸå¤± - ç¡®ä¿è¾“å‡ºåˆ†å¸ƒçš„ç¨³å®šæ€§
            # # è®¡ç®—è·¨batchçš„æ–¹å·®ï¼Œå¸Œæœ›åŒä¸€å±‚çº§çš„è¾“å‡ºç›¸å¯¹ç¨³å®š
            # consistency_loss = torch.var(cross_scores, dim=0, unbiased=False).mean()  # è·¨ä¸“å®¶ç»´åº¦çš„æ–¹å·®å‡å€¼
            
            #æ ¹æ®ä¸åŒå±‚çº§çš„ç‰¹æ€§ï¼Œè°ƒæ•´æŸå¤±æƒé‡ï¼ˆåœ¨å¯¹åº”çš„level_losså‰é¢ä¹˜ä¸Šä¸swinçš„å…·ä½“æ‰€åœ¨å±‚æ•°ç›¸å…³çš„ç³»æ•°ï¼‰
            #lowéšswinå±‚æ•°å¢å¤§æƒé‡å‡å°ï¼Œmediumå…ˆå¢åå‡ï¼Œhighéšswinå±‚æ•°å¢å¤§æƒé‡å¢å¤§
            # å•å±‚çº§æŸå¤±
            level_loss = recon_loss 
            # + self.gamma1 * focus_loss 
            # + self.gamma2 * consistency_loss
            total_loss += level_loss
            
        return total_loss / 3  # ä¸‰ä¸ªå±‚çº§çš„å¹³å‡æŸå¤±

